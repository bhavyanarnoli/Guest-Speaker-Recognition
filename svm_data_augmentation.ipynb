{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install audiomentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install audiomentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyrubberband in c:\\users\\chief engineer (c)\\appdata\\roaming\\python\\python310\\site-packages (0.3.0)\n",
      "Requirement already satisfied: six in c:\\users\\chief engineer (c)\\anaconda3\\lib\\site-packages (from pyrubberband) (1.16.0)\n",
      "Requirement already satisfied: pysoundfile>=0.8.0 in c:\\users\\chief engineer (c)\\appdata\\roaming\\python\\python310\\site-packages (from pyrubberband) (0.9.0.post1)\n",
      "Requirement already satisfied: cffi>=0.6 in c:\\users\\chief engineer (c)\\anaconda3\\lib\\site-packages (from pysoundfile>=0.8.0->pyrubberband) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\chief engineer (c)\\anaconda3\\lib\\site-packages (from cffi>=0.6->pysoundfile>=0.8.0->pyrubberband) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyrubberband\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from librosa.core.spectrum import _spectrogram\n",
    "import os\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "from audiomentations import AddBackgroundNoise, PolarityInversion\n",
    "from audiomentations import Compose, AddGaussianNoise, PitchShift, HighPassFilter, AddGaussianSNR,AddShortNoises\n",
    "import random\n",
    "from sklearn import mixture\n",
    "import pickle\n",
    "import pyrubberband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_folders = [\n",
    "    \"Benjamin_Netanyau\",\n",
    "    \"Jens_Stoltenberg\",\n",
    "    \"Julia_Gillard\",\n",
    "    \"Magaret_Tarcher\",\n",
    "    \"Nelson_Mandela\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_save_audio(path,speaker):\n",
    "    files = os.listdir(path)\n",
    "    # files = sorted(files)\n",
    "    # print(sorted(files))\n",
    "    combined_audio = []\n",
    "    temp_sr = 0\n",
    "    for i in range(len(files)):\n",
    "        # print(files[i])\n",
    "        file_path = path + '/' + str(i) + '.wav'\n",
    "        x,sr = librosa.load(file_path)\n",
    "        temp_sr = sr\n",
    "        combined_audio.extend(x)\n",
    "    output_file_path = '../ML_project/combined_audios/' + speaker + '.wav'\n",
    "    sf.write(output_file_path,combined_audio,temp_sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:35<00:00,  7.20s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(os.listdir('../ML_project/16000_pcm_speeches')):\n",
    "    combine_and_save_audio('../ML_project/16000_pcm_speeches/'+i,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_audio_with_noises(path,speaker):\n",
    "    x,sr = librosa.load(path)\n",
    "    transform = AddBackgroundNoise(\n",
    "        sounds_path=\"../ML_project/_background_noise_\",\n",
    "        min_snr_in_db=3.0,\n",
    "        max_snr_in_db=30.0,\n",
    "        noise_transform=PolarityInversion(),\n",
    "        p=1.0)\n",
    "    augmented_sound = transform(x, sample_rate=sr)\n",
    "    output_file_path = '../ML_project/combindes_audio_with_noises/' + speaker + '.wav'\n",
    "    sf.write(output_file_path,augmented_sound,sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def calculate_features(x, sr):\n",
    "    x_pre_emp = librosa.effects.preemphasis(x)\n",
    "    x_mfcc = librosa.feature.mfcc(y=x_pre_emp,sr=sr,n_mfcc=13)\n",
    "    delta_mfcc = librosa.feature.delta(x_mfcc)\n",
    "    dissimilarities = distance.cdist(x_mfcc.T, x_mfcc.T, 'euclidean')\n",
    "    ldb_features = np.mean(dissimilarities, axis=1)\n",
    "    combined = np.vstack((x_mfcc,delta_mfcc, ldb_features))\n",
    "    return combined.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_features(x,sr=22050):\n",
    "#     x_pre_emp = librosa.effects.preemphasis(x)\n",
    "#     x_mfcc = librosa.feature.mfcc(y=x_pre_emp,sr=sr,n_mfcc=13)\n",
    "#     delta_mfcc = librosa.feature.delta(x_mfcc)\n",
    "#     double_delta_mfcc = librosa.feature.delta(delta_mfcc)\n",
    "#     combined = np.vstack((x_mfcc,delta_mfcc,double_delta_mfcc))\n",
    "#     return combined.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.uniform(3,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio_extract_features(path, n_steps, rate, flag_value):\n",
    "    x, sr = librosa.load(path)\n",
    "    \n",
    "    augmented_pitch_shift = librosa.effects.pitch_shift(x, sr=sr, n_steps=n_steps)\n",
    "    augmented_time_stretch = librosa.effects.time_stretch(x, rate = rate)\n",
    "\n",
    "    transform_BGN = AddBackgroundNoise(\n",
    "        sounds_path=\"../ML_project/_background_noise_\",\n",
    "        min_snr_in_db=3.0,\n",
    "        max_snr_in_db=30.0,\n",
    "        noise_transform=PolarityInversion(),\n",
    "        p=1.0)\n",
    "    \n",
    "    augmented_noise = transform_BGN(x, sample_rate=sr)\n",
    "    \n",
    "    add_gaussian_white_noise_transform = AddGaussianSNR(\n",
    "        min_snr_db=5.0,\n",
    "        max_snr_db=40.0,\n",
    "        p=0.2\n",
    "    )\n",
    "    \n",
    "    add_short_noises_transform = AddShortNoises(\n",
    "        sounds_path=\"../ML_project/combined_audios\",\n",
    "        min_snr_in_db=3.0,\n",
    "        max_snr_in_db=30.0,\n",
    "        noise_rms=\"relative_to_whole_input\",\n",
    "        min_time_between_sounds=2.0,\n",
    "        max_time_between_sounds=8.0,\n",
    "        noise_transform=PolarityInversion(),\n",
    "        p=0.5\n",
    "    )\n",
    "    \n",
    "    add_gaussian_white_noise = add_gaussian_white_noise_transform(x, sr)\n",
    "    add_short_noises = add_short_noises_transform(x, sr)\n",
    "\n",
    "    features_x = calculate_features(x, sr)\n",
    "    pitch_shift_features = calculate_features(augmented_pitch_shift, sr)\n",
    "    \n",
    "    # Concatenate the original and time-stretched signals\n",
    "    concatenated_time_stretch = np.concatenate([x, augmented_time_stretch])\n",
    "    \n",
    "    # Calculate features for the concatenated signal\n",
    "    time_stretch_features = calculate_features(concatenated_time_stretch, sr)\n",
    "    \n",
    "    augmented_noise_feature = calculate_features(augmented_noise, sr)\n",
    "\n",
    "    if flag_value > 0.5:\n",
    "        white_noise_features = calculate_features(add_gaussian_white_noise, sr)\n",
    "        short_voices_feature = calculate_features(add_short_noises, sr)\n",
    "        return np.vstack((features_x, white_noise_features, pitch_shift_features, \n",
    "                          time_stretch_features, augmented_noise_feature, short_voices_feature))\n",
    "    return np.vstack((features_x, pitch_shift_features, time_stretch_features, augmented_noise_feature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_test_audio_randomly(path,n_steps,rate):\n",
    "    x,sr = librosa.load(path)\n",
    "    augmented_pitch_shift = librosa.effects.pitch_shift(x, sr=sr, n_steps=n_steps)\n",
    "\n",
    "\n",
    "    augmented_time_stretch =librosa.effects.time_stretch(x, rate = rate)\n",
    "\n",
    "\n",
    "    transform_BGN = AddBackgroundNoise(\n",
    "        sounds_path=\"../ML_project/_background_noise_\",\n",
    "        min_snr_in_db=3.0,\n",
    "        max_snr_in_db=30.0,\n",
    "        noise_transform=PolarityInversion(),\n",
    "        p=1.0)\n",
    "    augmented_noise = transform_BGN(x, sample_rate=sr)\n",
    "    add_gaussian_white_noise_transform = AddGaussianSNR(\n",
    "        min_snr_db=5.0,\n",
    "        max_snr_db=40.0,\n",
    "        p=0.2\n",
    "    )\n",
    "    add_short_noises_transform = AddShortNoises(\n",
    "        sounds_path=\"../ML_project/combined_audios\",\n",
    "        min_snr_in_db=3.0,\n",
    "        max_snr_in_db=30.0,\n",
    "        noise_rms=\"relative_to_whole_input\",\n",
    "        min_time_between_sounds=2.0,\n",
    "        max_time_between_sounds=8.0,\n",
    "        noise_transform=PolarityInversion(),\n",
    "        p=0.5\n",
    "    )\n",
    "    add_gaussian_white_noise = add_gaussian_white_noise_transform(x,sr)\n",
    "    add_short_noises = add_short_noises_transform(x,sr)\n",
    "    \n",
    "    features_x = calculate_features(x,sr)\n",
    "    pitch_shift_features = calculate_features(augmented_pitch_shift,sr)\n",
    "    time_stretch_features = calculate_features(augmented_time_stretch,sr)\n",
    "    augmented_noise_feature = calculate_features(augmented_noise,sr)\n",
    "\n",
    "    white_noise_features = calculate_features(add_gaussian_white_noise,sr)\n",
    "    short_voices_feature = calculate_features(add_short_noises,sr)\n",
    "    a = np.random.randint(1,6)\n",
    "    if a==1:\n",
    "        return features_x\n",
    "    elif a==2:\n",
    "        return white_noise_features\n",
    "    elif a==3:\n",
    "        return short_voices_feature\n",
    "    elif a==4:\n",
    "        return pitch_shift_features\n",
    "    elif a==5:\n",
    "        return time_stretch_features\n",
    "    return augmented_noise_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benjamin_Netanyau\n",
      "Jens_Stoltenberg\n",
      "Julia_Gillard\n",
      "Magaret_Tarcher\n",
      "Nelson_Mandela\n"
     ]
    }
   ],
   "source": [
    "feature_space = []\n",
    "feature_labels = []\n",
    "for Class in os.listdir('../ML_project/16000_pcm_speeches'):\n",
    "    print(Class)\n",
    "    folder_data = os.listdir('../ML_project/16000_pcm_speeches'+'/'+Class)\n",
    "    # split_20_data = int(len(folder_data) * 0.2)\n",
    "    # split_80 = len(folder_data) - split_20_data\n",
    "    # list_20 = random.sample(folder_data, split_20_data)\n",
    "    # list_80 = [elem for elem in folder_data if elem not in list_20]\n",
    "    # data_dict[i] = folder_data\n",
    "    # test_dict[i] = list_80\n",
    "    \n",
    "    for file in folder_data:\n",
    "        n_steps = random.uniform(-4,4)\n",
    "        rate = random.uniform(0.7,1.3)\n",
    "        flag_value = random.uniform(0,1)\n",
    "        file_path = '../ML_project/16000_pcm_speeches' + '/' + Class + '/' + file\n",
    "        curr_fea = augment_audio_extract_features(file_path,n_steps=n_steps,rate=rate,\\\n",
    "                                                flag_value=flag_value)\n",
    "        feature_space.append(curr_fea)\n",
    "        feature_labels.append(Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_space, feature_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n",
      "209\n"
     ]
    }
   ],
   "source": [
    "def data_shape_correction(data_set):\n",
    "    for i in range(len(data_set)):\n",
    "        data_set[i] = data_set[i].T[:,:209]\n",
    "    return data_set\n",
    "\n",
    "X_train = data_shape_correction(X_train)\n",
    "chotu = X_train[0].shape[1]\n",
    "badu = X_train[0].shape[1]\n",
    "for i in X_train:\n",
    "    if chotu > i.shape[1]:\n",
    "        chotu = i.shape[1]\n",
    "    if badu < i.shape[1]:\n",
    "        badu = i.shape[1]\n",
    "print(chotu)\n",
    "print(badu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 209)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(len(X_train)):\n",
    "#     X_train[i] = X_train[i].T[:209]\n",
    "    \n",
    "# print(len(X_train[4]))\n",
    "X_train = np.array(X_train)\n",
    "X_train_flattened = X_train.reshape(X_train.shape[0], -1)\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel='rbf')\n",
    "print(type(X_train_flattened))\n",
    "classifier.fit(X_train_flattened, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 5643)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 27, 209)\n"
     ]
    }
   ],
   "source": [
    "# Use the trained SVM to predict labels for the test set\n",
    "X_test = data_shape_correction(X_test)\n",
    "X_test = np.array(X_test)\n",
    "X_test_flattened = X_test.reshape(X_test.shape[0], -1)\n",
    "print((X_train.shape))\n",
    "y_pred = classifier.predict(X_test_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.60%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming 'model' is your trained model\n",
    "with open('svm_mfcc_ldb_column_sliced_209.pkl', 'wb') as file:\n",
    "    pickle.dump(classifier, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9660\n",
      "Precision: 0.9661\n",
      "Recall: 0.9660\n",
      "F1 Score: 0.9659\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Benjamin_Netanyau       0.94      0.94      0.94       296\n",
      " Jens_Stoltenberg       0.98      0.93      0.96       302\n",
      "    Julia_Gillard       0.96      0.98      0.97       299\n",
      "  Magaret_Tarcher       0.95      0.97      0.96       326\n",
      "   Nelson_Mandela       0.99      1.00      1.00       277\n",
      "\n",
      "         accuracy                           0.97      1500\n",
      "        macro avg       0.97      0.97      0.97      1500\n",
      "     weighted avg       0.97      0.97      0.97      1500\n",
      "\n",
      "Confusion Matrix:\n",
      " [[279   5   3   9   0]\n",
      " [  9 282   7   4   0]\n",
      " [  1   0 294   2   2]\n",
      " [  7   1   1 317   0]\n",
      " [  0   0   0   0 277]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true=y_test, y_pred=y_pred, average='weighted')\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true=y_test, y_pred=y_pred, average='weighted')\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true=y_test, y_pred=y_pred, average='weighted')\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(y_true=y_test, y_pred=y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyrubberband\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = '../ML_project/Models2'\n",
    "# models = [pickle.load(open(model_path+'/'+fname,'rb')) for fname in os.listdir(model_path)]\n",
    "# speakers   = [fname.split('.')[0] for fname in os.listdir(model_path)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [08:25<00:00, 101.08s/it]\n"
     ]
    }
   ],
   "source": [
    "y_test = []\n",
    "y_pred = []\n",
    "for Class,x in tqdm(test_dict.items()):\n",
    "    for file in x:\n",
    "      n_steps = random.uniform(-4,4)\n",
    "      rate = random.uniform(0.7,1.3)\n",
    "      file_path = '../ML_project/16000_pcm_speeches' + '/' + Class + '/' + file\n",
    "      curr_fea = augment_test_audio_randomly(file_path,n_steps,rate)\n",
    "      # print(curr_fea.shape)\n",
    "      y_test.append(Class)\n",
    "      log_likelihood = np.zeros(len(models)) \n",
    "    #   curr_fea = curr_fea.reshape(1,-1)\n",
    "      for i in range(len(models)):\n",
    "          gmm = models[i]         #checking with each model one by one\n",
    "          scores = np.array(gmm.score(curr_fea))\n",
    "          log_likelihood[i] = scores.sum()\n",
    "    \n",
    "      winner = np.argmax(log_likelihood)\n",
    "      y_pred.append(speakers[winner])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9906666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_true=y_test,y_pred=y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
